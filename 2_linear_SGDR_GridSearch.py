# -*- coding: utf-8 -*-
"""ML_sec1,2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ubQmG_whVzNUYiVuysOcoFRnBFCR_6M
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency, f_oneway
from matplotlib.colors import LinearSegmentedColormap

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/ML data/FuelConsumption.csv')
df.info()

df.describe()

df.drop(columns=['MODELYEAR'], axis = 1, inplace=True)

df.sample(3)

df.duplicated().sum()

df.isna().sum().sum()

df.columns[:-1]

cat_cols = []
num_cols = []
for col in df.columns[:-1]:
  col_type = df[col].dtype
  if col_type == 'object' or col_type == 'string':
    cat_cols.append(col)
    print(col, ':')
    print(f'{len(df[col].unique())} =')
    # print(df[col].unique())
    print('_'*100)
  elif col_type == 'datetime64':
    continue
  else:
    num_cols.append(col)
# num_cols , cat_cols

corr_matrix = df.loc[:, num_cols].corr()
colors = [
    (0.0, "#0D3B66"),
    (0.25, "white"),
    (0.75, "white"),
    (1.0, "#0D3B66")
]
cmap = LinearSegmentedColormap.from_list('', colors)
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap=cmap, vmax=1, vmin=-1)
plt.show()
corr_matrix_filtered = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k =1).astype('bool'))
corr_matrix_filtered = corr_matrix_filtered[corr_matrix_filtered>0.8].stack()

corr_matrix_filtered = corr_matrix_filtered.reset_index().rename(columns={'level_0':'feature1', 'level_1':'feature2', 0:'corr'})
corr_matrix_filtered = corr_matrix_filtered.sort_values(by='corr',ascending=False)

f1 = []
f2 = []
for feature1 in corr_matrix_filtered['feature1']:
   f1.append(df[feature1].corr(df['CO2EMISSIONS']))
for feature2 in corr_matrix_filtered['feature2']:
  f2.append(df[feature2].corr(df['CO2EMISSIONS']))
corr_matrix_filtered['targetCorrWithF1'] = f1
corr_matrix_filtered['targetCorrWithF2'] = f2
corr_matrix_filtered.reset_index(drop=True, inplace=True)
corr_matrix_filtered

corr_all = []
for col in num_cols:
  corr_all.append(df[col].corr(df['CO2EMISSIONS']))
corr_all

df.drop(columns=['FUELCONSUMPTION_COMB','FUELCONSUMPTION_HWY','ENGINESIZE'], axis=1, inplace=True)

def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    r, k = confusion_matrix.shape
    return (chi2/(n*(min(k-1, r-1))))**0.5

z = len(cat_cols)
crosstabs = np.zeros((z,z))
crosstabs = pd.DataFrame(crosstabs , index=cat_cols, columns=cat_cols)
for col1 in cat_cols:
  for col2 in cat_cols:
    crosstabs.loc[col1,col2] = cramers_v(df[col1],df[col2])

crosstabs_filtered = crosstabs.where(np.triu(np.ones(crosstabs.shape),k =1).astype('bool'))
crosstabs_filtered = crosstabs_filtered[crosstabs_filtered>0.8].stack()
crosstabs_filtered

sns.heatmap(crosstabs, annot=True, fmt=".2f", cmap=cmap, vmax=1, vmin=-1)
plt.show()

result = []
for col in cat_cols:
  groups = [y for x,y in df.groupby(col)['CO2EMISSIONS']]
  f_stat, _ = f_oneway(*groups)
  result.append(f_stat)
result = pd.DataFrame(result, index=cat_cols, columns=['F-statistic'])
result

df.drop(columns=['MODEL','MAKE','TRANSMISSION'], axis=1, inplace=True)

df.sample(3)

cat_cols = []
num_cols = []
for col in df.columns[:-1]:
  col_type = df[col].dtype
  if col_type == 'object' or col_type == 'string':
    cat_cols.append(col)
  else:
    num_cols.append(col)
num_cols,cat_cols

i = 1
df_count = len(df)
plt.figure(figsize=(11,7),dpi=100)

for col in num_cols:
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower = Q1 - 1.5*IQR
  upper = Q3 + 1.5*IQR
  outliers = df[(df[col] < lower) | (df[col] > upper)][col]
  n_outliers = len(outliers)

  plots = int(np.ceil(len(num_cols)/4))
  plot_n =int(f'{plots}4{i}')
  plt.subplot(plot_n)
  sns.boxplot(df[col])
  plt.xlabel(f'{col}')
  plt.title(f'outliers: {n_outliers}/{df_count}')
  i += 1
plt.tight_layout(pad=7)
plt.show()

"""از انجایی که مقادیر اوتلایر ها مربوط به ماشینهای صنعتی هستند. با فرض اینکه میخواهیم در تحلیل ها بر روی این گروه ماشین ها نیز تحلیل انجام شود. آن ها را نگه میداریم.

نرمال سازی با توجه به آنکه روش آن بر اساس الگوریتم انتخابی متفاوت است و بر روی داده های تست فقط میخواهیم ترنزفرم کنیم و هنوز دیتای تست و ترین معلوم نگردیده است از انجام در این مرحله صرف نظر شده است.
"""

df = pd.get_dummies(df, columns=['FUELTYPE'], drop_first=False, dtype=int)
cat_cols.remove('FUELTYPE')
df.sample(2)

!pip install category_encoders

import category_encoders as ce
loo_encoder = ce.LeaveOneOutEncoder(cols=cat_cols)
df_encoded = loo_encoder.fit_transform(df[cat_cols], df['CO2EMISSIONS'])
new_cols = [col + '_LOO' for col in cat_cols]
df[new_cols] = df_encoded

df.sample(2)

encoded_cols = [c for c in df.columns if '_LOO' in c or 'FUELTYPE_' in c]
target_col = ['CO2EMISSIONS']
new_order = encoded_cols + num_cols + target_col + cat_cols

df = df[new_order]

df.sample(2)

X = df.iloc[:,0:7].values
y = df.iloc[:,8].values
X, y

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_validate, KFold, GridSearchCV

# finding best heyper parameters for batch gradian descent
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('SGDR', SGDRegressor())
])
param_grid = {
    'SGDR__max_iter': [500, 2000,3000],
    'SGDR__eta0': [0.0001, 0.001, 0.01, 0.1],
    'SGDR__alpha': [0.0001, 0.001, 0.01, 0.1],
    'SGDR__penalty': ['l1', 'l2', 'elasticnet']
}
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X, y)
best_params = grid_search.best_params_
best_params, grid_search.best_score_

best_params

models = [LinearRegression(),SGDRegressor(**{k.split('__')[1]:v for k,v in best_params.items()})]
evaluation_df = pd.DataFrame(np.zeros((2,2)), columns=['MAPE','R2'], index=['n_equation','SGDR'])
kfold = KFold(n_splits=5, shuffle=True)
for i, model in enumerate(models):
  pipeline = Pipeline([
      ('scaler', StandardScaler()),
      ('model', model)
  ])
  cross_validation = cross_validate(pipeline,X,y,cv=kfold, scoring=['neg_mean_absolute_percentage_error', 'r2'])
  evaluation_df.iloc[i,:] =[-cross_validation['test_neg_mean_absolute_percentage_error'].mean(), cross_validation['test_r2'].mean()]


evaluation_df
